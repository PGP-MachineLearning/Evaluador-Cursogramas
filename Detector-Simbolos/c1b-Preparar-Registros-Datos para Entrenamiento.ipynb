{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"c1b-Preparar-Registros-Datos para Entrenamiento.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pHefTSRioYTI"},"source":["# Genera los TFRecords en base a los XMLs correspondientes a las imágenes\n","\n","Adaptado de https://github.com/douglasrizzo/detection_util_scripts \n","\n","Fuente: https://medium.com/@omcar17/how-to-convert-xml-files-into-tfrecords-in-tensorflow2-0-86120b553f0b\n"]},{"cell_type":"markdown","metadata":{"id":"jKsjtRyL56Zm"},"source":["0) Preparar el ambiente:"]},{"cell_type":"code","metadata":{"id":"f_Yx7m1smq3m","cellView":"form"},"source":["#@title Actualizar e instalar paquetes necesarios\n","!pip install -U --pre tensorflow==\"2.*\"\n","!pip install tf_slim\n","!pip install pycocotools"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95ItJmL3mtnM","cellView":"form"},"source":["#@title Clonar el repositorio de modelos de TF si no está ya disponible\n","import os\n","import pathlib\n","\n","if \"models\" in pathlib.Path.cwd().parts:\n","  while \"models\" in pathlib.Path.cwd().parts:\n","    os.chdir('..')\n","elif not pathlib.Path('models').exists():\n","  !git clone --depth 1 https://github.com/tensorflow/models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMFzDgfjmu81","cellView":"form"},"source":["#@title Instalar el Object Detection API\n","# Nota: si falla por falta de requerimientos, ejecutarlo de nuevo y funcionará ;)...\n","%%bash\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","cp object_detection/packages/tf2/setup.py .\n","python -m pip install ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ne-dkOmFmx9J","cellView":"form"},"source":["#@title Re-instalar el Object Detection API\n","# Nota: se ejecuta de nuevo para que lo instale bien\n","%%bash\n","cd models/research/\n","protoc object_detection/protos/*.proto --python_out=.\n","cp object_detection/packages/tf2/setup.py .\n","python -m pip install ."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hACYrusvgi2D"},"source":["1) Define las librerías a utilizar:"]},{"cell_type":"code","metadata":{"id":"5YFhdM6_giDP","cellView":"form"},"source":["#@title Cargar Librerías\n","import os\n","import glob\n","import pandas as pd\n","\n","import xml.etree.ElementTree as ET\n","\n","import io\n","import tensorflow as tf\n","\n","from PIL import Image\n","from tqdm import tqdm\n","from object_detection.utils import dataset_util\n","from collections import namedtuple, OrderedDict\n","\n","print(\"Librerías cargadas.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XKXKZbK5RPQZ"},"source":["2) Monta el Drive y define archivos a utilizar:"]},{"cell_type":"code","metadata":{"id":"eiEs_Q3K6WN5","cellView":"form"},"source":["#@title Montar Drive y definir archivos a procesar\n","\n","# monta Google Drive nuevamente (se pierde conexión cuando se reinicia el entrorno anterior)\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# configuración de directorios local en Google Drive\n","drive_path = '/content/gdrive/My Drive/GEMIS/objDetectionCursogramas'\n","data_dir_path = drive_path + '/Cursogramas'\n","\n","print(\"\\n\")\n","print(\"> Datos disponibles en: \", data_dir_path)\n","print(\"\\n\")\n","\n","# define las carpetas de XMLs\n","train_image_dir = data_dir_path + '/train/images'\n","train_xml_dir = data_dir_path + '/train/annotations'\n","test_image_dir = data_dir_path + '/validation/images'\n","test_xml_dir = data_dir_path + '/validation/annotations' \n","print(\"> train_image_dir: \", train_image_dir)\n","print(\"> train_xml_dir: \", train_xml_dir)\n","print(\"> test_image_dir: \", test_image_dir)\n","print(\"> test_xml_dir: \", test_xml_dir)\n","print(\"\\n\")\n","\n","# define los nombres de los archivos\n","train_csv_fname = data_dir_path + '/train_labels.csv'\n","test_csv_fname = data_dir_path + '/test_labels.csv'\n","print(\"> train_csv_fname: \", train_csv_fname)\n","print(\"> test_csv_fname: \", test_csv_fname)\n","\n","label_map_pbtxt_fname = data_dir_path + '/label_map.pbtxt'\n","print(\"> label_map_pbtxt_fname: \", label_map_pbtxt_fname)\n","\n","train_record_fname = data_dir_path + '/train.record'\n","test_record_fname = data_dir_path + '/test.record'\n","print(\"> train_record_fname: \", train_record_fname)\n","print(\"> test_record_fname: \", test_record_fname)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DglxS28FgtD9"},"source":["3) Genera CSVs auxiliares en base a los XMLs correspondientes a las imágenes: "]},{"cell_type":"code","metadata":{"id":"Y8QdAlf7hZvP","cellView":"form"},"source":["#@title Definir funciones auxiliares para CSV\n","def __list_to_csv(annotations, output_file):\n","    column_name = [\n","        'filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax'\n","    ]\n","    xml_df = pd.DataFrame(annotations, columns=column_name)\n","    xml_df.to_csv(output_file, index=None)\n","\n","def xml_to_csv(xml_dir, output_file):\n","    \"\"\"Reads all XML files, generated by labelImg, from a directory and generates a single CSV file\"\"\"\n","    annotations = []\n","    for xml_file in glob.glob(xml_dir + '/*.xml'):\n","        tree = ET.parse(xml_file)\n","        root = tree.getroot()\n","        for member in root.findall('object'):\n","            value = (root.find('filename').text,\n","                     int(root.find('size')[0].text),\n","                     int(root.find('size')[1].text), member[0].text,\n","                     int(member[4][0].text), int(member[4][1].text),\n","                     int(member[4][2].text), int(member[4][3].text))\n","            annotations.append(value)\n","\n","    __list_to_csv(annotations, output_file)\n","\n","print(\"Funciones auxiliares para CSV definidas\")    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PsUMKhqvg-Uq","cellView":"form"},"source":["#@title Generar CSVs\n","\n","# Por las dudas, se elimina los archivos si ya existen (para asegurar su actualización)\n","if os.path.isfile(train_csv_fname):\n","  os.remove(train_csv_fname)\n","  print(train_csv_fname, \" eliminado.\")\n","\n","if os.path.isfile(test_csv_fname):\n","  os.remove(test_csv_fname)\n","  print(test_csv_fname, \" eliminado.\")\n","\n","# genera CSVs\n","print(\"\\n\")\n","print(\"-- generando train_csv: \", train_csv_fname)\n","xml_to_csv(train_xml_dir, train_csv_fname)\n","print(\"> train_csv generado.\")\n","print(\"\\n\")\n","print(\"-- generando test_csv: \", test_csv_fname)\n","xml_to_csv(test_xml_dir, test_csv_fname)\n","print(\"> test_csv generado.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJX2F0VMjLyL"},"source":["4) Genera archivo de clases label_map_pbtxt:"]},{"cell_type":"code","metadata":{"id":"HLgC2gYpjdZv","cellView":"form"},"source":["#@title Definir funciones auxiliares para label_map\n","def pbtxt_from_csv(csv_path, pbtxt_path):\n","    class_list = list(pd.read_csv(csv_path)['class'].unique())\n","    class_list.sort()\n","\n","    pbtxt_from_classlist(class_list, pbtxt_path)\n","\n","\n","def pbtxt_from_classlist(l, pbtxt_path):\n","    pbtxt_text = ''\n","\n","    for i, c in enumerate(l):\n","        pbtxt_text += 'item {\\n    id: ' + str(\n","            i + 1) + '\\n    display_name: \"' + c + '\"\\n}\\n\\n'\n","\n","    with open(pbtxt_path, \"w+\") as pbtxt_file:\n","        pbtxt_file.write(pbtxt_text)\n","\n","print(\"Funciones auxiliares para label_map definidas\")    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISTK9v_vjMCp","cellView":"form"},"source":["#@title Generar label_map_pbtx\n","\n","#Por las dudas, se elimina los archivos si ya existen (para asegurar su actualización)\n","if os.path.isfile(label_map_pbtxt_fname):\n","  os.remove(label_map_pbtxt_fname)\n","  print(label_map_pbtxt_fname, \" eliminado.\")\n","\n","print(\"\\n\")\n","print(\"-- generando label_map_pbtxt: \", label_map_pbtxt_fname)\n","pbtxt_from_csv(train_csv_fname, label_map_pbtxt_fname)\n","print(\"> label_map_pbtxt generado.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djpz2beJ7TdC"},"source":["5) Generar TFRecords en base a los XMLs correspondientes a las imágenes:"]},{"cell_type":"code","metadata":{"id":"XwCGiNR9kaqF","cellView":"form"},"source":["#@title Definir funciones auxiliares para TFRecords\n","def generate_TFrecords(pbtxt_input, csv_input, image_dir, output_path):\n","    class_dict = class_dict_from_pbtxt(pbtxt_input)\n","\n","    writer = tf.io.TFRecordWriter(output_path)\n","    path = os.path.join(image_dir)\n","    examples = pd.read_csv(csv_input)\n","    grouped = __split(examples, 'filename')\n","\n","    for group in tqdm(grouped, desc='groups'):\n","        tf_example = create_tf_example(group, path, class_dict)\n","        writer.write(tf_example.SerializeToString())\n","\n","    writer.close()\n","\n","def create_tf_example(group, path, class_dict):\n","    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)),\n","                        'rb') as fid:\n","        encoded_jpg = fid.read()\n","    encoded_jpg_io = io.BytesIO(encoded_jpg)\n","    image = Image.open(encoded_jpg_io)\n","    width, height = image.size\n","\n","    filename = group.filename.encode('utf8')\n","    image_format = b'jpg'\n","    xmins = []\n","    xmaxs = []\n","    ymins = []\n","    ymaxs = []\n","    classes_text = []\n","    classes = []\n","\n","    for index, row in group.object.iterrows():\n","        if set(['xmin_rel', 'xmax_rel', 'ymin_rel', 'ymax_rel']).issubset(\n","                set(row.index)):\n","            xmin = row['xmin_rel']\n","            xmax = row['xmax_rel']\n","            ymin = row['ymin_rel']\n","            ymax = row['ymax_rel']\n","\n","        elif set(['xmin', 'xmax', 'ymin', 'ymax']).issubset(set(row.index)):\n","            xmin = row['xmin'] / width\n","            xmax = row['xmax'] / width\n","            ymin = row['ymin'] / height\n","            ymax = row['ymax'] / height\n","\n","        xmins.append(xmin)\n","        xmaxs.append(xmax)\n","        ymins.append(ymin)\n","        ymaxs.append(ymax)\n","        classes_text.append(row['class'].encode('utf8'))\n","        classes.append(class_dict[row['class']])\n","\n","    tf_example = tf.train.Example(\n","        features=tf.train.Features(\n","            feature={\n","                'image/height':\n","                dataset_util.int64_feature(height),\n","                'image/width':\n","                dataset_util.int64_feature(width),\n","                'image/filename':\n","                dataset_util.bytes_feature(filename),\n","                'image/source_id':\n","                dataset_util.bytes_feature(filename),\n","                'image/encoded':\n","                dataset_util.bytes_feature(encoded_jpg),\n","                'image/format':\n","                dataset_util.bytes_feature(image_format),\n","                'image/object/bbox/xmin':\n","                dataset_util.float_list_feature(xmins),\n","                'image/object/bbox/xmax':\n","                dataset_util.float_list_feature(xmaxs),\n","                'image/object/bbox/ymin':\n","                dataset_util.float_list_feature(ymins),\n","                'image/object/bbox/ymax':\n","                dataset_util.float_list_feature(ymaxs),\n","                'image/object/class/text':\n","                dataset_util.bytes_list_feature(classes_text),\n","                'image/object/class/label':\n","                dataset_util.int64_list_feature(classes),\n","            }))\n","    return tf_example\n","\n","def __split(df, group):\n","    data = namedtuple('data', ['filename', 'object'])\n","    gb = df.groupby(group)\n","    return [\n","        data(filename, gb.get_group(x))\n","        for filename, x in zip(gb.groups.keys(), gb.groups)\n","    ]\n","\n","def class_dict_from_pbtxt(pbtxt_path):\n","    # open file, strip \\n, trim lines and keep only\n","    # lines beginning with id or display_name\n","    data = [\n","        l.rstrip('\\n').strip()\n","        for l in open(pbtxt_path, 'r', encoding='utf-8-sig')\n","        if 'id:' in l or 'display_name:'\n","    ]\n","    ids = [int(l.replace('id:', '')) for l in data if l.startswith('id')]\n","    names = [\n","        l.replace('display_name:', '').replace('\"', '').strip() for l in data\n","        if l.startswith('display_name')\n","    ]\n","\n","    #print(data)\n","\n","    # join ids and display_names into a single dictionary\n","    class_dict = {}\n","    for i in range(len(ids)):\n","        class_dict[names[i]] = ids[i]\n","\n","    return class_dict\n","\n","\n","print(\"Funciones auxiliares para TFRecords definidas\")   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kplWBZNd8us0","cellView":"form"},"source":["#@title Generar TFRecords\n","\n","# por las dudas, se elimina los archivos si ya existen (para asegurar su actualización)\n","if os.path.isfile(train_record_fname):\n","  os.remove(train_record_fname)\n","  print(train_record_fname, \" eliminado.\")\n","\n","if os.path.isfile(test_record_fname):\n","  os.remove(test_record_fname)\n","  print(test_record_fname, \" eliminado.\")\n","\n","print(\"\\n\")\n","print(\"-- generando train_TFRecords: \", train_record_fname)\n","generate_TFrecords(label_map_pbtxt_fname, train_csv_fname, train_image_dir, train_record_fname)\n","print(\"\\n> train_TFRecords generado.\")\n","print(\"\\n\")\n","print(\"-- generando test_TFRecords: \", test_record_fname)\n","generate_TFrecords(label_map_pbtxt_fname, test_csv_fname, test_image_dir, test_record_fname)\n","print(\"\\n> test_TFRecords generado.\")"],"execution_count":null,"outputs":[]}]}